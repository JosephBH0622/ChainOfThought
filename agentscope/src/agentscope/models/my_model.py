import json
import time
from abc import ABC
from typing import Any, Union, Sequence, List

import requests
from loguru import logger

from .model import ModelWrapperBase, ModelResponse
from ..constants import _DEFAULT_MAX_RETRIES
from ..constants import _DEFAULT_MESSAGES_KEY
from ..constants import _DEFAULT_RETRY_INTERVAL
from ..message import MessageBase
from ..utils.tools import _convert_to_str


class MyModelWrapperBase(ModelWrapperBase, ABC):
    model_type: str = "my_Llama3"

    def __init__(
            self,
            config_name,
            api_url: str,
            headers: dict = None,
            max_length: int = 2048,
            timeout: int = 30,
            json_args: dict = None,
            post_args: dict = None,
            max_retries: int = _DEFAULT_MAX_RETRIES,
            messages_key: str = _DEFAULT_MESSAGES_KEY,
            retry_interval: int = _DEFAULT_RETRY_INTERVAL,
            **kwargs: Any,
    ) -> None:
        # 初始化模型实例
        super().__init__(config_name=config_name)
        self.api_url = api_url
        self.headers = headers
        self.max_length = max_length
        self.timeout = timeout
        self.json_args = json_args or {}
        self.post_args = post_args or {}
        self.max_retries = max_retries
        self.messages_key = messages_key
        self.retry_interval = retry_interval

    def _parse_response(self, response: str) -> ModelResponse:
        """Parse the response json data into ModelResponse"""
        return ModelResponse(raw=response)

    # def truncate_response(self, response, identifier="<|eot_id|>"):
    #     text = response["choices"][0]["message"]["content"]
    #     match = re.search(re.escape(identifier), text)
    #     if match:
    #         text = text[:match.start()]
    #         response["choices"][0]["message"]["content"] = text
    #         return response
    #     return response

    def truncate_response(self, response, identifier="<|eot_id|>"):
        content = ""
        for i, line in enumerate(response.iter_lines(decode_unicode=True)):
            if line:
                line = line[5:].strip()
                json_data = json.loads(line)
                content_ = json_data["choices"][0]['delta']['content']
                if content_ == identifier:
                    break
                content += content_
        return content

    def __call__(self, input_: str, **kwargs) -> ModelResponse:
        """
        Calling the model with requests.post.

        Args:
            input_ (`str`):
                The input string to the model.

        Returns:
            `dict`: A dictionary that contains the response of the model and
            related
            information (e.g. cost, time, the number of tokens, etc.).

        Note:
            `parse_func`, `fault_handler` and `max_retries` are reserved for
            `_response_parse_decorator` to parse and check the response
            generated by model wrapper. Their usages are listed as follows:
                - `parse_func` is a callable function used to parse and check
                the response generated by the model, which takes the response
                as input.
                - `max_retries` is the maximum number of retries when the
                `parse_func` raise an exception.
                - `fault_handler` is a callable function which is called
                when the response generated by the model is invalid after
                `max_retries` retries.
        """
        # Step1: prepare keyword auguments
        # post_args = {**self.post_args, **kwargs}
        if isinstance(self.json_args, dict):
            keys = self.json_args.keys()
            values = self.json_args.values()
            model_key = list(keys)[0]
            model_val = list(values)[0]
            stream_key = list(keys)[1]
            stream_val = list(values)[1]
            request_kwargs = {
                "url": self.api_url,
                "json": {model_key: model_val, self.messages_key: input_, stream_key: stream_val},
                "headers": self.headers or {},
            }
        else:
            request_kwargs = {
                "url": self.api_url,
                "json": {self.messages_key: input_, **self.json_args},
                "headers": self.headers or {},
            }

        # Step2: prepare post requests
        # for i in range(1, self.max_retries + 1):
        response = requests.post(**request_kwargs)
        content = self.truncate_response(response)

            # if response.status_code == requests.codes.ok:
            #     break
            #
            # if i < self.max_retries:
            #     logger.warning(
            #         f"Failed to call the model with "
            #         f"requests.codes == {response.status_code}, retry "
            #         f"{i + 1}/{self.max_retries} times",
            #     )
            #     time.sleep(i * self.retry_interval)

        # Step3: record model invocation
        # record the model api invocation, which will be skipped if
        # 'FileManager.save_api_invocation' is 'False'
        self._save_model_invocation(
            arguments=request_kwargs,
            response=content,
        )

        # Step4: parse the response
        if response.status_code == requests.codes.ok:
            return self._parse_response(content)
        else:
            logger.error(json.dumps(request_kwargs, indent=4))
            raise RuntimeError(
                f"Failed to call the model with {content}",
            )


class MyModelWrapper(MyModelWrapperBase):
    """
    A post api model wrapper compatible with Llama3 chat.
    """

    model_type: str = "my_Llama3_chat"

    def _parse_response(self, response: str) -> ModelResponse:
        # print(response)
        return ModelResponse(
            text=response
        )

    # def _parse_response(self, response: dict) -> ModelResponse:
    #     """Parse the response json data into ModelResponse"""
    #     return ModelResponse(raw=response)

    def format(
            self,
            *args: Union[MessageBase, Sequence[MessageBase]],
    ) -> Union[List[dict]]:
        """
        Format the input messages into a dict, which is compatible to Llama3 Chat API.

        Args:
            args (`Union[MessageBase, Sequence[MessageBase]]`):
                The input arguments to be formatted, where each argument
                should be a `Msg` object, or a list of `Msg` objects.
                In distribution, placeholder is also allowed.

        Returns:
            `Union[dict]`:
                The formatted messages.
        """
        messages = []
        for arg in args:
            if arg is None:
                continue
            if isinstance(arg, MessageBase):
                messages.append(
                    {
                        "role": arg.role,
                        "content": _convert_to_str(arg.content),
                    },
                )
            elif isinstance(arg, list):
                messages.extend(self.format(*arg))
            else:
                raise TypeError(
                    f"The input should be Msg object or a list "
                    f"of Msg objects, got {type(arg)}.",
                )
        return messages
